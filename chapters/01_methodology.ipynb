{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Methodology\n",
    "\n",
    "## Research Design\n",
    "\n",
    "This study employs a controlled experimental design to compare the statistical properties of LLM outputs generated through two different methods:\n",
    "\n",
    "1. **Batch Generation**: Using the `n` parameter to generate multiple completions in a single API call\n",
    "2. **Sequential Generation**: Making separate API calls for each completion\n",
    "\n",
    "Our hypothesis is that these methods may produce statistically different distributions, violating the common assumption that LLM outputs are independent and identically distributed (i.i.d.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "### Models Tested\n",
    "- OpenAI GPT-4o-mini\n",
    "- OpenAI GPT-4\n",
    "- Google Gemini Pro (using `candidateCount` parameter)\n",
    "\n",
    "### Test Prompts\n",
    "We use three categories of prompts to test different aspects of model behavior:\n",
    "\n",
    "1. **Random Number Generation**: \"Pick a random number between 1 and 100.\"\n",
    "2. **Classification Tasks**: Binary sentiment analysis\n",
    "3. **Creative Generation**: Short story beginnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "from experiments import NParameterExperiment, ExperimentConfig\n",
    "from analysis import IndependenceAnalyzer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Tests\n",
    "\n",
    "We employ multiple statistical tests to assess independence and distributional equivalence:\n",
    "\n",
    "### 1. Kolmogorov-Smirnov Test\n",
    "Tests whether two samples come from the same distribution.\n",
    "\n",
    "$$D_{n,m} = \\sup_x |F_{1,n}(x) - F_{2,m}(x)|$$\n",
    "\n",
    "where $F_{1,n}$ and $F_{2,m}$ are the empirical distribution functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def demonstrate_ks_test():\n",
    "    \"\"\"Demonstrate KS test on synthetic data.\"\"\"\n",
    "    from scipy.stats import ks_2samp\n",
    "    \n",
    "    # Same distribution\n",
    "    sample1 = np.random.normal(50, 10, 100)\n",
    "    sample2 = np.random.normal(50, 10, 100)\n",
    "    \n",
    "    ks_stat, p_value = ks_2samp(sample1, sample2)\n",
    "    print(f\"Same distribution: KS={ks_stat:.4f}, p={p_value:.4f}\")\n",
    "    \n",
    "    # Different distributions\n",
    "    sample3 = np.random.normal(55, 10, 100)\n",
    "    ks_stat, p_value = ks_2samp(sample1, sample3)\n",
    "    print(f\"Different distributions: KS={ks_stat:.4f}, p={p_value:.4f}\")\n",
    "\n",
    "demonstrate_ks_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Position Effects Analysis\n",
    "\n",
    "We test whether the position within a batch affects the output distribution. This is critical because prior research suggests potential within-batch dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def visualize_position_effects():\n",
    "    \"\"\"Visualize how position effects might manifest.\"\"\"\n",
    "    # Simulate data with position effects\n",
    "    n_batches = 20\n",
    "    batch_size = 5\n",
    "    \n",
    "    # Create data where position 0 tends higher\n",
    "    data = []\n",
    "    for batch in range(n_batches):\n",
    "        for position in range(batch_size):\n",
    "            # Add position-dependent bias\n",
    "            value = 50 + (2 - position) * 3 + np.random.normal(0, 5)\n",
    "            data.append({\n",
    "                'batch': batch,\n",
    "                'position': position,\n",
    "                'value': value\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Box plot by position\n",
    "    df.boxplot(column='value', by='position', ax=ax1)\n",
    "    ax1.set_title('Value Distribution by Position')\n",
    "    ax1.set_xlabel('Position in Batch')\n",
    "    ax1.set_ylabel('Value')\n",
    "    \n",
    "    # Mean value by position\n",
    "    position_means = df.groupby('position')['value'].mean()\n",
    "    ax2.plot(position_means.index, position_means.values, 'o-')\n",
    "    ax2.set_title('Mean Value by Position')\n",
    "    ax2.set_xlabel('Position in Batch')\n",
    "    ax2.set_ylabel('Mean Value')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_position = visualize_position_effects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Variance Decomposition\n",
    "\n",
    "We decompose the total variance into within-batch and between-batch components:\n",
    "\n",
    "$$\\sigma^2_{total} = \\sigma^2_{within} + \\sigma^2_{between}$$\n",
    "\n",
    "A high within/between ratio suggests position effects or other within-batch dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_variance_components(df):\n",
    "    \"\"\"Analyze variance components from position effects data.\"\"\"\n",
    "    # Overall variance\n",
    "    overall_var = df['value'].var()\n",
    "    \n",
    "    # Within-batch variance (average variance within each batch)\n",
    "    within_vars = df.groupby('batch')['value'].var()\n",
    "    mean_within_var = within_vars.mean()\n",
    "    \n",
    "    # Between-batch variance (variance of batch means)\n",
    "    batch_means = df.groupby('batch')['value'].mean()\n",
    "    between_var = batch_means.var()\n",
    "    \n",
    "    print(f\"Overall variance: {overall_var:.2f}\")\n",
    "    print(f\"Mean within-batch variance: {mean_within_var:.2f}\")\n",
    "    print(f\"Between-batch variance: {between_var:.2f}\")\n",
    "    print(f\"Within/Between ratio: {mean_within_var/between_var:.2f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    components = ['Overall', 'Within-Batch', 'Between-Batch']\n",
    "    values = [overall_var, mean_within_var, between_var]\n",
    "    \n",
    "    ax.bar(components, values, color=['blue', 'green', 'red'])\n",
    "    ax.set_ylabel('Variance')\n",
    "    ax.set_title('Variance Decomposition')\n",
    "    \n",
    "    for i, v in enumerate(values):\n",
    "        ax.text(i, v + 1, f'{v:.2f}', ha='center')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "analyze_variance_components(df_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Intraclass Correlation Coefficient (ICC)\n",
    "\n",
    "The ICC measures the proportion of variance that is due to between-group differences:\n",
    "\n",
    "$$ICC = \\frac{\\sigma^2_{between}}{\\sigma^2_{between} + \\sigma^2_{within}}$$\n",
    "\n",
    "High ICC values (e.g., 0.69 as found by Gallo et al., 2025) indicate strong within-batch correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Implications\n",
    "\n",
    "### Effect on Sample Size Calculations\n",
    "\n",
    "When outputs are correlated, the effective sample size is reduced by the design effect:\n",
    "\n",
    "$$n_{effective} = \\frac{n_{actual}}{1 + (m-1) \\times ICC}$$\n",
    "\n",
    "where $m$ is the cluster size (batch size).\n",
    "\n",
    "For example, with ICC = 0.69 and batch size = 10:\n",
    "- Design effect = 1 + (10-1) × 0.69 = 7.21\n",
    "- 100 samples → effective n ≈ 14\n",
    "- **86% reduction in statistical power!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_design_effect(icc, cluster_size, total_samples):\n",
    "    \"\"\"Calculate the design effect and effective sample size.\"\"\"\n",
    "    design_effect = 1 + (cluster_size - 1) * icc\n",
    "    n_effective = total_samples / design_effect\n",
    "    \n",
    "    print(f\"ICC: {icc}\")\n",
    "    print(f\"Cluster size: {cluster_size}\")\n",
    "    print(f\"Total samples: {total_samples}\")\n",
    "    print(f\"Design effect: {design_effect:.2f}\")\n",
    "    print(f\"Effective sample size: {n_effective:.1f}\")\n",
    "    print(f\"Power reduction: {(1 - n_effective/total_samples)*100:.1f}%\")\n",
    "    \n",
    "    # Visualize impact across different ICC values\n",
    "    icc_values = np.linspace(0, 0.9, 50)\n",
    "    effective_sizes = [total_samples / (1 + (cluster_size - 1) * icc) \n",
    "                      for icc in icc_values]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(icc_values, effective_sizes, linewidth=2)\n",
    "    plt.axhline(y=total_samples, color='gray', linestyle='--', label='Actual n')\n",
    "    plt.axvline(x=0.69, color='red', linestyle='--', label='Gallo et al. (2025) ICC')\n",
    "    plt.xlabel('Intraclass Correlation Coefficient (ICC)')\n",
    "    plt.ylabel('Effective Sample Size')\n",
    "    plt.title(f'Impact of ICC on Effective Sample Size (cluster size={cluster_size})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example from Gallo et al. (2025)\n",
    "calculate_design_effect(icc=0.69, cluster_size=10, total_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This methodology allows us to:\n",
    "\n",
    "1. **Detect position effects**: Identify if outputs vary systematically by position within batch\n",
    "2. **Measure correlation**: Quantify the degree of within-batch dependence\n",
    "3. **Test distributional differences**: Determine if batch and sequential methods produce different distributions\n",
    "4. **Calculate research impact**: Estimate the effect on statistical power and confidence intervals\n",
    "\n",
    "The next chapter will apply this methodology to real LLM outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}